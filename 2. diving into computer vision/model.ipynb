{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0ca01fa",
   "metadata": {},
   "source": [
    "# A neural network to classify images of clothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd1a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03628bfe",
   "metadata": {},
   "source": [
    "We're using a dataset called 'Fashion MNIST'. GitHub: https://github.com/zalandoresearch/fashion-mnist.\n",
    "\n",
    "This dataset contains 60,000 training images and 10,000 testing images. Each image is a 28x28px grayscale image of a fashion item, associated with a label from 10 classes. The task is to predict the class of the image (e.g. t-shirt , dress, etc.). Each image has a resolution of 28x28 pixels and is grayscale (1 channel). The grayscale values range from 0 to 255, where 0 represents black and 255 represents white.\n",
    "\n",
    "| Label | Item |\n",
    "|-------|------|\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |\n",
    "<p></p>\n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <div style=\"margin-right: 20px;\">\n",
    "        <img src=\"mnist.png\" width=400px alt=\"mnist dataset image\">\n",
    "    </div>\n",
    "    <div>\n",
    "        <img src=\"mnist.gif\" width=600px alt=\"mnist classes\">\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538e6c73",
   "metadata": {},
   "source": [
    "## Loading data and normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abbda46",
   "metadata": {},
   "source": [
    "> `tfds.load` loads the data.\n",
    "\n",
    "> `split` specifies which splits of the dataset is to be loaded. You set as_supervised to True to ensure that the loaded tf.data.Dataset will have a 2-tuple structure (input, label).\n",
    "\n",
    "> `ds_train` and `ds_test` are of type `tf.data.Dataset`. `ds_train` has 60,000 images which will be used for training the model. `ds_test` has 10,000 images which will be used for evaluating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "024db661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, load and configure data\n",
    "(ds_train, ds_test), info = tfds.load('fashion_mnist', split=['train', 'test'], with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "143fdbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 10\n",
      "Number of training examples: 60000\n",
      "Number of test examples: 10000\n"
     ]
    }
   ],
   "source": [
    "header = info.features['label'].num_classes\n",
    "print(f\"Number of classes: {header}\")\n",
    "print(f\"Number of training examples: {info.splits['train'].num_examples}\")\n",
    "print(f\"Number of test examples: {info.splits['test'].num_examples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bbc9bfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before normalization -> 0 227\n"
     ]
    }
   ],
   "source": [
    "# Minium and maximum values before normalization\n",
    "image_batch, labels_batch = next(iter(ds_train))\n",
    "print(\"Before normalization ->\", np.min(image_batch[0]), np.max(image_batch[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715494da",
   "metadata": {},
   "source": [
    "> `Batch size` refers to the number of training examples utilized in one iteration.\n",
    "\n",
    "We'll be normalizing the data to exist between 0 and 1.\n",
    "\n",
    "code given below uses the `map()` function of `tf.data.Dataset` to apply the normalization to images in `ds_train` and `ds_test`. Since the pixel values are of type `tf.uint8`, the `tf.cast` function is used to convert them to `tf.float32` and then divide by 255.0. The dataset is also converted into batches by calling the `batch()` method with BATCH_SIZE as the argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "323df650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51d006fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and batch process the dataset\n",
    "ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y)).batch(BATCH_SIZE)\n",
    "ds_test = ds_test.map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bee7eb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After normalization -> 0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# Min and max values of the batch after normalization\n",
    "image_batch, labels_batch = next(iter(ds_train))\n",
    "print(\"After normalization ->\", np.min(image_batch[0]), np.max(image_batch[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c538b2ca",
   "metadata": {},
   "source": [
    "## Design, Compile and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dfa298aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c4a0e6",
   "metadata": {},
   "source": [
    "0. Sequential\n",
    "\n",
    "- This is the default mode. It means that the model will process the input data in a sequential manner, i.e., one sample at a time. This is the most common mode and is suitable for most use cases.\n",
    "\n",
    "\n",
    "1. Flatten Layer\n",
    "\n",
    "- Converts the 2D image array (28x28 pixels) into a 1D array (784 pixels)\n",
    "- No parameters to learn, just reshapes the data\n",
    "- Input: (28, 28) → Output: (784)\n",
    "\n",
    "2. Dense (addds a layer of neurons) Hidden Layer\n",
    "- 64 neurons in this hidden layer\n",
    "- ReLU activation function (Rectified Linear Unit)\n",
    "- Takes flattened input (784) and learns patterns\n",
    "- Parameters: 784 * 64 + 64 = 50,240 (weights + biases)\n",
    "\n",
    "3. Output Layer\n",
    "- 10 neurons (one for each clothing class)\n",
    "- Softmax activation for probability distribution\n",
    "- Parameters: 64 * 10 + 10 = 650 (weights + biases)\n",
    "\n",
    "Each layer of neurons needs an activation function to decide if a neuron should be activated or not. There are lots of options, but this lab uses the following ones.\n",
    "- `Relu` effectively means if X>0 return X, else return 0. It passes values 0 or greater to the next layer in the network.\n",
    "- `Softmax` takes a set of values, and effectively picks the biggest one so you don't have to sort to find the largest value. For example, if the output of the last layer looks like [0.1, 0.1, 0.05, 0.1, 9.5, 0.1, 0.05, 0.05, 0.05], it returns [0,0,0,0,1,0,0,0,0].\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"network_1.svg\" width=\"600px\" alt=\"\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e0d725",
   "metadata": {},
   "source": [
    "The network architecture consists of:\n",
    "- Input: 28x28 grayscale image\n",
    "- Flatten → Dense(64) → Dense(10)\n",
    "- Total trainable parameters: 50,890\n",
    "\n",
    "Output is a probability distribution across 10 clothing classes:\n",
    "- T-shirt/top, Trouser, Pullover, Dress, Coat etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33a6cd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - loss: 0.6598 - sparse_categorical_accuracy: 0.7733\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.3989 - sparse_categorical_accuracy: 0.8594\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.3588 - sparse_categorical_accuracy: 0.8728\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.3337 - sparse_categorical_accuracy: 0.8800\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.3158 - sparse_categorical_accuracy: 0.8847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x23097d6b430>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "model.fit(ds_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf21d47b",
   "metadata": {},
   "source": [
    "## Evaluating models's performance on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b207f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3556 - sparse_categorical_accuracy: 0.8769\n",
      "\n",
      "Test accuracy: 0.8715\n",
      "Test loss: 0.3637\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on test data\n",
    "test_loss, test_accuracy = model.evaluate(ds_test)\n",
    "print(f\"\\nTest accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2527db52",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "402679e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m50,240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">152,672</span> (596.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m152,672\u001b[0m (596.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,890</span> (198.79 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m50,890\u001b[0m (198.79 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,782</span> (397.59 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m101,782\u001b[0m (397.59 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m50,240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">152,672</span> (596.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m152,672\u001b[0m (596.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,890</span> (198.79 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m50,890\u001b[0m (198.79 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,782</span> (397.59 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m101,782\u001b[0m (397.59 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the entire model as a Keras model using .keras format\n",
    "model.save('saved_model.keras') \n",
    "\n",
    "# Load the model using custom_objects to handle the custom activation function\n",
    "new_model = tf.keras.models.load_model('saved_model.keras', custom_objects={'softmax_v2': tf.keras.activations.softmax})\n",
    "\n",
    "# Summary of loaded SavedModel\n",
    "new_model.summary()\n",
    "\n",
    "# Save the entire model to a keras file.\n",
    "model.save('my_model.keras')\n",
    "\n",
    "# Recreate the exact same model, including its weights and the optimizer\n",
    "new_model_keras = tf.keras.models.load_model('my_model.keras', custom_objects={'softmax_v2': tf.keras.activations.softmax})\n",
    "\n",
    "# Summary of loaded keras model\n",
    "new_model_keras.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
