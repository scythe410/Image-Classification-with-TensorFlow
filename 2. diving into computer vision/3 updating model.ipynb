{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d1ffd62",
   "metadata": {},
   "source": [
    "## Exploring layers\n",
    "\n",
    "### 1. What happens if we change the number of neurons?\n",
    "\n",
    "When you increase to 128 neurons, you have to do more calculations. This slows down the training process. Here, the increase had a positive impact because the model is more accurate. But, it's `not always a case of 'more is better'`. You can hit the law of diminishing returns very quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8da7aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow_datasets\n",
    "import tensorflow_datasets as tfds\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "# Import TensorFlow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ceba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, load and configure data\n",
    "(ds_train, ds_test), info = tfds.load('fashion_mnist', split=['train', 'test'], with_info=True, as_supervised=True)\n",
    "\n",
    "\n",
    "# Define batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "# Normalizing and batch processing of data\n",
    "ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y)).batch(BATCH_SIZE)\n",
    "ds_test = ds_test.map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y)).batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "# Define the model (64 -> 128)\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "\n",
    "\n",
    "# Compile data\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "model.fit(ds_train, epochs=5)\n",
    "\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c67843",
   "metadata": {},
   "source": [
    "### 2. What will happen if we add another layer between the two dense layers?\n",
    "\n",
    "> Answer: No significant impact - because this is relatively simple data. For far more complex data, extra layers are often necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8c9773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, load and configure data\n",
    "(ds_train, ds_test), info = tfds.load('fashion_mnist', split=['train', 'test'], with_info=True, as_supervised=True)\n",
    "\n",
    "\n",
    "# Define batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "# Normalizing and batch processing of data\n",
    "ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y)).batch(BATCH_SIZE)\n",
    "ds_test = ds_test.map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y)).batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "\n",
    "\n",
    "# Compile data\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "model.fit(ds_train, epochs=5)\n",
    "\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c7b7d6",
   "metadata": {},
   "source": [
    "### 3. Before, we normalized the pixel values to the range of [0, 1]. What would be the impact of removing normalization so that the values are in the range of [0, 255], like they were originally in the dataset?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bed186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, load and configure data\n",
    "(ds_train, ds_test), info = tfds.load('fashion_mnist', split=['train', 'test'], with_info=True, as_supervised=True)\n",
    "\n",
    "# Define batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Normalizing and batch processing of data\n",
    "ds_train = ds_train.batch(BATCH_SIZE)\n",
    "ds_test = ds_test.batch(BATCH_SIZE)\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e78de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out max value to see the changes\n",
    "image_batch, labels_batch = next(iter(ds_train))\n",
    "t_image_batch, t_labels_batch = next(iter(ds_test))\n",
    "print(\"training images max:\", np.max(image_batch[0]))\n",
    "print(\"test images max:\", np.max(t_image_batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beb71f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile data\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "model.fit(ds_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47025364",
   "metadata": {},
   "source": [
    "There is a significant reduction in the accuracy.\n",
    "\n",
    "> **Does Normalization always increase the accuracy?**\n",
    "\n",
    "> Apparently, No. It is not necessary that normalization always increases accuracy. It may or might not, you never really know until you implement. Again it depends on at which stage in you training you apply normalization, on whether you apply normalization after every activation, etc. As the range of the values of the features gets narrowed down to a particular range because of normalization, its easy to perform computations over a smaller range of values. So, usually the model gets trained a bit faster. Regarding the number of epochs, accuracy usually increases with number of epochs provided that your model doesn't start over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df6b8e2",
   "metadata": {},
   "source": [
    "### 4. What happens if you remove the Flatten() layer, and why?\n",
    "\n",
    "> Answer: We get an error about the shape of the data. Expected.\n",
    "\n",
    "I'm not really sure how this part works yet :)\n",
    "\n",
    "But when when the arrays are loaded into the model later, they'll automatically be flattened for us if we use Flatten()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057e65a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, load and configure data\n",
    "(ds_train, ds_test), info = tfds.load('fashion_mnist', split=['train', 'test'], with_info=True, as_supervised=True)\n",
    "\n",
    "\n",
    "# Define batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "# Normalizing and batch processing of data\n",
    "ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y)).batch(BATCH_SIZE)\n",
    "ds_test = ds_test.map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y)).batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "# Define the model- removed Flatten()\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "\n",
    "\n",
    "# Compile data\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "model.fit(ds_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb24a9d6",
   "metadata": {},
   "source": [
    "### 5. What happens if we change the last no of neurons from 10 to something else?\n",
    "\n",
    "> **the number of neurons in the last layer should match the number of classes you we classifying for**\n",
    "\n",
    "So when running, we get an error as soon as it finds an unexpected value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efa7b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, load and configure data\n",
    "(ds_train, ds_test), info = tfds.load('fashion_mnist', split=['train', 'test'], with_info=True, as_supervised=True)\n",
    "\n",
    "\n",
    "# Define batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "# Normalizing and batch processing of data\n",
    "ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y)).batch(BATCH_SIZE)\n",
    "ds_test = ds_test.map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y)).batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "# Define the model (10 -> 5)\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(5, activation=tf.nn.softmax)])\n",
    "\n",
    "\n",
    "# Compile data\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "model.fit(ds_train, epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
